{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping for flats\n",
    "\n",
    "Get some lists from ``leboncoin.fr``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import urllib\n",
    "import unidecode\n",
    "import numpy as np\n",
    "from string import atof,atoi\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "#             get the 1st 100\n",
    "base_url='https://www.leboncoin.fr/locations/offres/ile_de_france/paris/'\n",
    "\n",
    "# ----------------------------------------\n",
    "#            a shortcut\n",
    "urlget = urllib.urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downlad page number \n",
    "\n",
    "The default search criteria are for flats with size > 30 sqm, in Paris.\n",
    "The function returns the `Beautifulsoup`-ped HTML of a given page.\n",
    "Each page contains a list of flats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.leboncoin.fr/locations/offres/ile_de_france/paris/?o=2&sqs=2&ret=1&ret=2&ret=5\n"
     ]
    }
   ],
   "source": [
    "def get_page_from_url(pgnr,base_url):\n",
    "    CL_paris_apa_html=\"{0:s}?o={1:d}&sqs=2&ret=1&ret=2&ret=5\".format(base_url,pgnr)\n",
    "    print CL_paris_apa_html\n",
    "    try:\n",
    "        xpage = urlget(CL_paris_apa_html)\n",
    "        CL_html = xpage.read()\n",
    "        encoding = xpage.info()['content-type'].split('=')[-1]\n",
    "    except IOError: # in case of network error\n",
    "        print 'IOError'\n",
    "        CL_html = ''\n",
    "    try:\n",
    "        return BS(CL_html.decode(encoding,'replace'),'lxml')\n",
    "    except UnboundLocalError:\n",
    "        return BS(CL_html.decode('latin-1','replace'),'lxml')\n",
    "human_page=get_page_from_url(2,base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List flats urls from page\n",
    "\n",
    "From each page given by the previous function, we record the list of flats, their publication date, and pid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_listings_from_html(human_page,base_url):\n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    yesterday = yesterday.strftime('%d/%m/%y')\n",
    "    today = today.strftime('%d/%m/%y')\n",
    "\n",
    "    apartments = human_page.findAll('a',attrs={'class':'list_item'})\n",
    "    \n",
    "    results=[]\n",
    "    for item in apartments:\n",
    "        try:\n",
    "            title=unidecode.unidecode(''.join(item.select('h2.item_title')[0].contents).strip())\n",
    "        except IndexError:\n",
    "            continue\n",
    "        insert_time = item.select('aside.item_absolute > p.item_supp')\n",
    "        if not len(insert_time):\n",
    "            continue\n",
    "        insert_time = insert_time[0].contents[-1].strip().replace(u\"Aujourd'hui\",today)\n",
    "        insert_time = insert_time.replace(u\"Hier\",yesterday)\n",
    "        url = 'http:'+item.attrs['href']\n",
    "        pid = url.split('/')[-1].split('.')[0]\n",
    "        results.append({'insert_time':insert_time,'pid':pid,'url':url,'title':title})\n",
    "        md5_input = title\n",
    "        results[-1].update({'md5sum':hashlib.md5(md5_input).hexdigest()})\n",
    "    \n",
    "    return results\n",
    "apas=get_listings_from_html(human_page,base_url)\n",
    "# apas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for jix in xrange(len(flats)):\n",
    "#     keys=['insert_time','pid','url']\n",
    "#     fl_ = {j:flats[jix] for j in keys}\n",
    "#     flats[jix].update({'md5sum':hashlib.md5(str(fl)).hexdigest()})\n",
    "    \n",
    "# print flats[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From flat url to flat record\n",
    "\n",
    "We have the urls of all flats listed in CL.. well, we have the functions to get the urls, the actual work will be done at the end.\n",
    "Finally, we can get the informations out of each flat individual page. This is really boring, but needed. The results is a list of features for each flat. These features will need further refinement, that I will do in future notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def select2text(din):\n",
    "    basic_string = [' '.join(list(j.stripped_strings)) for j in din]\n",
    "#     basic_string =  [unidecode.unidecode(j) for j in basic_string]\n",
    "    return basic_string\n",
    "\n",
    "def make_col_name(nm):\n",
    "    nm = unidecode.unidecode(nm)\n",
    "    nm = nm.title()\n",
    "    for sub in \" ,./;'[]\\<>?:{}|=+!@$%^&*()_-#\":\n",
    "        nm = nm.replace(sub,'')\n",
    "    if nm[:2]=='Pi' and nm[-2:]=='es':\n",
    "        nm='Pieces'\n",
    "    if 'Meubl' in nm:\n",
    "        nm = 'Meuble'\n",
    "    return nm.strip()\n",
    "    \n",
    "# functions to process the raw features record\n",
    "from collections import defaultdict as DDict\n",
    "def default():\n",
    "    return lambda out:out\n",
    "\n",
    "def loyer(out):\n",
    "    # process loyer\n",
    "    lm = out['LoyerMensuel']\n",
    "    lm = make_col_name(lm)\n",
    "    eur = lm.split('Eur')[0].replace(' ','')\n",
    "    out['LoyerMensuel']=atof(eur)\n",
    "    # add a filed \n",
    "    out['ChargesComprises'] = False\n",
    "    if 'ChargesComprises' in lm:\n",
    "        out['ChargesComprises'] = True\n",
    "    return out\n",
    "\n",
    "def surface(out):\n",
    "    # process surface\n",
    "    sr = out['Surface']\n",
    "    sr = atof(sr.strip().split('m')[0])\n",
    "    out['Surface'] = sr\n",
    "    return out\n",
    "    \n",
    "def meublenonmeuble(out):\n",
    "    #process MeubleNonMeuble\n",
    "    mnm = out['Meuble']\n",
    "    if 'Non' in mnm:\n",
    "        out['Meuble'] = False\n",
    "    else:\n",
    "        out['Meuble'] = True\n",
    "    return out\n",
    "\n",
    "def ville(out):\n",
    "    #process Ville\n",
    "    vl = out['Ville']\n",
    "    zipcode = atoi(vl.split()[-1])\n",
    "    arrondissement = zipcode - 75000\n",
    "    out['Arrondissement'] = arrondissement\n",
    "    return out\n",
    "    \n",
    "unrawify_dict=DDict(default)\n",
    "unrawify_dict['Ville']=ville\n",
    "unrawify_dict['Meuble']=meublenonmeuble\n",
    "unrawify_dict['Surface']=surface\n",
    "unrawify_dict['LoyerMensuel']=loyer\n",
    "\n",
    "def unrawify_apas(indict):\n",
    "    outdict=indict.copy()\n",
    "    for j in indict.keys():\n",
    "        outdict.update(unrawify_dict[j](indict))\n",
    "    return outdict\n",
    "\n",
    "def apafeatures(apainfo):\n",
    "    url = apainfo['url']\n",
    "    apa=BS(urlget(url).read())\n",
    "    col=select2text(apa.select('section.properties .property'))\n",
    "    col = map(make_col_name,col)\n",
    "    \n",
    "    val=select2text(apa.select('section.properties .value'))\n",
    "    \n",
    "    lines = min(len(col),len(val))\n",
    "    out = {}\n",
    "    for i in xrange(lines):\n",
    "        out[col[i]]=val[i]\n",
    "    \n",
    "    out = unrawify_apas(out)\n",
    "    if len(val)!=len(col):\n",
    "        out['Problematic']=True\n",
    "    else:\n",
    "        out['Problematic']=False\n",
    "        \n",
    "    \n",
    "    out.update(apainfo)\n",
    "    return out\n",
    "# print apas[5]\n",
    "# apafeatures(apas[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A memoizing page getter\n",
    "As the docstring says: `sort of memoizing for apafeatures, specialized for the apas tuple`.\n",
    "\n",
    "This class saves its cache as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pkl\n",
    "# from datetime import datetime as DT\n",
    "import os\n",
    "import time\n",
    "\n",
    "class get_features_cache(object):\n",
    "    \"\"\"\n",
    "        sort of memoizing for apafeatures, specialized for the flat record\n",
    "    \"\"\"\n",
    "    def __init__(self,fname,maxdt=-1):\n",
    "        self.fname=fname\n",
    "        try:\n",
    "            self.db=pkl.load(open(fname,'rb'))\n",
    "        except IOError:\n",
    "            self.db=[]\n",
    "        self.clids=[j['md5sum'] for j in self.db]\n",
    "        self.dirty = 0\n",
    "        self.upd_time = time.strftime('%H:%M %d/%m/%y')\n",
    "        \n",
    "    def __call__(self,apainfo):\n",
    "        url = apainfo['url']\n",
    "        clid = apainfo['md5sum']\n",
    "        ins_time = apainfo['insert_time']\n",
    "        # check if app was already retrieved\n",
    "        if clid in self.clids:\n",
    "            #TODO check if retrieved version is too old/invalid\n",
    "            return {'data':self.db[self.clids.index(clid)],'from_cache':True}\n",
    "        else:\n",
    "            #retrieve data, store, update self.clids, and finally return\n",
    "            out = apafeatures(apainfo)\n",
    "            \n",
    "            self.clids.append(clid)\n",
    "            self.db.append(out)\n",
    "            self.dirty = 1\n",
    "            self.upd_time = time.strftime('%H:%M %d/%m/%y')\n",
    "            return {'data':out,'from_cache':False}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self._save()\n",
    "        \n",
    "    def _save(self):\n",
    "        if self.dirty:\n",
    "            print \"saving apas db\"\n",
    "            print \"db rows: {0:d}\".format(len(self.db))\n",
    "            pkl.dump(self.db,open(self.fname,'wb'))\n",
    "            self.dirty = 0\n",
    "            \n",
    "    def upd_date(self):\n",
    "        return self.upd_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask pages at random times\n",
    "\n",
    "This is class that behaves as a function (see the `__call__`) that waits a random time before retrieving the requested URL.\n",
    "The waiting time is sampled from a Poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "#            Poissonian waiting time in the urlget function\n",
    "class urlgetter(object):\n",
    "    def __init__(self,waiting_time):\n",
    "        self.mean=waiting_time\n",
    "        \n",
    "    def __call__(self,url):\n",
    "        import time\n",
    "        #waiting time [s]\n",
    "        wt = self.poisson()\n",
    "        time.sleep(wt)\n",
    "        return urllib.urlopen(url)\n",
    "    \n",
    "    def poisson(self):\n",
    "        from math import log\n",
    "        from random import random\n",
    "        return -log(1.0 - random()) / self.mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all togheter\n",
    "\n",
    "The culprit of all these efforts, the loop that rules them all, where the  work is  truly done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 09:43 24/01/17 the db data/LBClocations.pkl contains 1008 locations\n",
      "https://www.leboncoin.fr/locations/offres/ile_de_france/paris/?o=1&sqs=2&ret=1&ret=2&ret=5\n",
      "page 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astyonax/.anaconda/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/astyonax/.anaconda/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.leboncoin.fr/locations/offres/ile_de_france/paris/?o=2&sqs=2&ret=1&ret=2&ret=5\n",
      "page 2\n",
      "https://www.leboncoin.fr/locations/offres/ile_de_france/paris/?o=3&sqs=2&ret=1&ret=2&ret=5\n",
      "page 3\n",
      "https://www.leboncoin.fr/locations/offres/ile_de_france/paris/?o=4&sqs=2&ret=1&ret=2&ret=5\n",
      "page 4\n",
      "https://www.leboncoin.fr/locations/offres/ile_de_france/paris/?o=5&sqs=2&ret=1&ret=2&ret=5\n",
      "page 5\n",
      "https://www.leboncoin.fr/locations/offres/ile_de_france/paris/?o=6&sqs=2&ret=1&ret=2&ret=5\n",
      "page 6\n",
      "saving apas db\n",
      "db rows: 1009\n",
      "last downloaded record:\n",
      "title: St-Placide 2 pieces meublees 35m2\n",
      "insert time: 22/01/17, 16:27\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "pages=range(1,7)\n",
    "# random.shuffle(pages)\n",
    "\n",
    "apagetter = get_features_cache(fname='data/LBClocations.pkl',)\n",
    "print 'The {2:s} the db {0:s} contains {1:d} locations'.format(apagetter.fname,len(apagetter),apagetter.upd_date())\n",
    "urlget = urlgetter(1/.5)\n",
    "failed=[]\n",
    "flats = []\n",
    "class Found(Exception): pass\n",
    "\n",
    "try:\n",
    "    for page in pages:\n",
    "        human_page=get_page_from_url(page,base_url)\n",
    "        apas=get_listings_from_html(human_page,base_url)\n",
    "        print 'page', page\n",
    "        if not len(apas): break\n",
    "        for count,apa in enumerate(apas):\n",
    "            try:\n",
    "                last=apagetter(apa)\n",
    "#                 print _['data']['title'],_['data']['insert_time']\n",
    "            except Exception,msg:\n",
    "                failed.append(apa)\n",
    "                print 'failed #{1:d} {0:s}'.format(apa,count)\n",
    "                print msg\n",
    "#             if _['from_cache']: raise Found\n",
    "except Found:\n",
    "    pass\n",
    "apagetter._save()\n",
    "print 'last downloaded record:'\n",
    "print 'title:',last['data']['title']\n",
    "print 'insert time:',last['data']['insert_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1009"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flats = apagetter.db\n",
    "len(flats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to the pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Arrondissement', u'ChargesComprises', u'ClasseEnergie',\n",
       "       u'ClasseInergie', u'ClasseShchnergie', u'ClasseYnergie', u'Description',\n",
       "       u'Ges', u'LoyerMensuel', u'Meuble', u'Pieces', u'Problematic',\n",
       "       u'Reference', u'Surface', u'TypeDeBien', u'Ville', u'insert_time',\n",
       "       u'md5sum', u'pid', u'title', u'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "df = pd.DataFrame(flats)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct_instime_format(j):\n",
    "    timefmt1 = '%d %b, %H:%M' # 25 oct, 11:59\n",
    "    timefmt2 = '%d/%m/%y, %H:%M' # 26/10/16, 19:17\n",
    "    outfmt   = '%d/%m/%y %H:%M'\n",
    "    \n",
    "    j = unidecode.unidecode(j)\n",
    "    try:\n",
    "        dt = datetime.strptime(j, timefmt1)\n",
    "        dt = dt.replace(year=datetime.today().year)\n",
    "        \n",
    "    except ValueError:\n",
    "        \n",
    "        try:\n",
    "            dt = datetime.strptime(j, timefmt2)\n",
    "        except ValueError:\n",
    "            \n",
    "            dt = datetime.strptime(j,outfmt)\n",
    "        \n",
    "    return dt.strftime(outfmt)\n",
    "df['insert_time']=df['insert_time'].apply(correct_instime_format)\n",
    "#######################################################################\n",
    "def safe_conv_pos_int(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "df['Pieces']=df['Pieces'].apply(safe_conv_pos_int)\n",
    "df_all=df.copy() #backup\n",
    "#######################################################################\n",
    "df['aux'] = pd.to_datetime(df['insert_time'])\n",
    "df['weekday']=df['aux'].apply(lambda x:x.weekday())\n",
    "df['ins_hour']=df['aux'].apply(lambda x:x.hour)\n",
    "#######################################################################\n",
    "# we care only of 2 pieces\n",
    "df=df[df['Pieces']==2]\n",
    "#######################################################################\n",
    "# remove if cheaper than 500 (it's crap) and bigger than 70\n",
    "df=df[(df.LoyerMensuel>500) & (df.Surface<70)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## work out some new columns\n",
    "- **Binning price by 250€**\n",
    "- **Binnig surface by 5 m<sup>2</sup>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface binning legend\n",
      "10 sqm -> # 3\n",
      "15 sqm -> # 4\n",
      "20 sqm -> # 5\n",
      "25 sqm -> # 6\n",
      "30 sqm -> # 7\n",
      "35 sqm -> # 8\n",
      "40 sqm -> # 9\n"
     ]
    }
   ],
   "source": [
    "i = [10,15,20,25,30,35,40]\n",
    "o = np.digitize(i,bins=np.arange(0,130,5))\n",
    "print 'Surface binning legend'\n",
    "for p,q in zip(i,o):\n",
    "    print p,'sqm -> #',q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "df['price_bin']=np.digitize(df.LoyerMensuel,bins=np.arange(0,21*250,250))\n",
    "df['sqm_bin'] = np.digitize(df.Surface,bins=np.arange(0,130,5))\n",
    "df['price_sqm'] = df.LoyerMensuel/df.Surface\n",
    "df = df[(df['price_sqm'] < 200)]\n",
    "#######################################################################\n",
    "mapges2int={'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8,'I':9,'N':10,'V':11,'X':12}\n",
    "def safe_ges(x):\n",
    "    try:\n",
    "        return x.strip()[0]\n",
    "    except AttributeError:\n",
    "        return 'X'\n",
    "    \n",
    "df['Ges_lit']=df['Ges'].apply(safe_ges)\n",
    "df['Ges_int']=df['Ges_lit'].apply(lambda x:mapges2int[x])\n",
    "########################################################################\n",
    "df['ClasseEnergie_lit']=df['ClasseEnergie'].apply(safe_ges)\n",
    "df['ClasseEnergie_int']=df['ClasseEnergie_lit'].apply(lambda x:mapges2int[x])\n",
    "########################################################################\n",
    "dfCC=df[df['ChargesComprises']==1]\n",
    "dfNC=df[df['ChargesComprises']==0]\n",
    "df=dfCC\n",
    "########################################################################\n",
    "def sistema_arrondissement(x):\n",
    "    if x>100:\n",
    "        return int(x-100)\n",
    "    else:\n",
    "        return int(x)\n",
    "df['Arrondissement']=df['Arrondissement'].apply(sistema_arrondissement)\n",
    "#######################################################################\n",
    "df['Meuble_int']=df['Meuble'].apply(safe_conv_pos_int)\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "407 flats recorded with 2 pieces, of size < 70sqm, and price >500eu\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "print \"*\"*100\n",
    "print \"{0:d} flats recorded with 2 pieces, of size < 70sqm, and price >500eu\".format(df.shape[0])\n",
    "print \"*\"*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the data for later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"data/lbc_pandas.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract position from descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def saveapply(x):\n",
    "#     try:\n",
    "#         x=x.replace(u'č',u'è')\n",
    "#         x=x.replace(u'ŕ',u'à')\n",
    "#         return unidecode.unidecode(x)\n",
    "#     except:\n",
    "#         return x\n",
    "    \n",
    "# def removeshort(x):\n",
    "#     symbols='\\'~!@#$%^&*()_+`-=[]\\{}|;:\"<>?,./'\n",
    "#     try:\n",
    "#         for j in symbols:\n",
    "#             x=x.replace(j,' ')\n",
    "#         return ' '.join([j for j in x.split() if len(j)>3])\n",
    "    \n",
    "#     except:\n",
    "#         return x\n",
    "\n",
    "# import cPickle as cpk\n",
    "# with open('data/metroParis.pkl','rb') as fin:\n",
    "#     metros = cpk.load(fin)\n",
    "# Mstat = set([unidecode.unidecode(j[0]) for j in metros])\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('data/positions-geographiques-des-stations-du-reseau-ratp.json','r') as fin:\n",
    "#     ratpinfo=json.load(fin)\n",
    "\n",
    "# def getratprecord(data,idx,what = [u'stop_name',u'coord']):\n",
    "#     root = u'fields'\n",
    "#     out= [data[idx][root][j] for j in what]\n",
    "#     for i,j in enumerate(out):\n",
    "#         try:\n",
    "#             out[i]=unidecode.unidecode(j)\n",
    "#         except:\n",
    "#             pass\n",
    "#     return out\n",
    "\n",
    "# tmp = [getratprecord(ratpinfo,i) for i in xrange(len(ratpinfo))][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import urllib2\n",
    "\n",
    "style=open('style.css','r').read()\n",
    "HTML(\"\"\"\n",
    "<style>{0}</style>\n",
    "\"\"\".format(style))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
